---
title: Telco customer churn
author:
  - name: Ketao Li
    affiliation: York University
    email:  liketao@yahoo.com
  - name: Kush Halani
    affiliation: York University
    email:  kush.halani@ontariotechu.net
  - name: Josue Romain
    affiliation: York University
    email:  josue.rolland.romain@gmail.com    
  - name: Juan Peña
    affiliation: York University
    email:  jppena62@my.yorku.ca
  - name: Priyanka Patil
    affiliation: York University
    email:  priyanka181994@gmail.com    
abstract: >
  Customer churn is a big challenge for large companies,especially in the highly competitive telecom industry. Due to the effect on the revenues of the companies, they are seeking to find ways to predict potential customer to churn. Therefore, identifying the factors that lead to customer churn is very important to take necessary actions to avoid this churn.Our work is to develop a churn prediction model which helps telecom operators to predict customers who are most likely subject to churn.

output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
---


```{r echo=FALSE, message=FALSE, warnings=FALSE}
# load required libraries
library(ggplot2) # plotting lib
library(gridExtra) # arrange grids
library(dplyr)  # data manipuation
library(mice)  # data imputing
library(corrplot) # correlation matrix plotting/printing
library(pROC) # to measure model performance
library(leaflet) # maps
library(RColorBrewer) # color palettes
library(VIM) # missing value analysis 
library(lattice) # another data plotting library
library(mapview) # saves map objects as file
library(png) # deals with png file measurements
library(knitr) #
library(party) # classification tree
library(klaR) # naive bayes
library(xtable) # tabular data formatting 
library(caret) # predictive models

# Clean all variables that might be left by other script to avoid collusion
rm(list=ls(all=TRUE))
# set xtable properties for the project
options(xtable.floating = TRUE)
options(xtable.timestamp = "")
options(xtable.comment = FALSE)

# pick palettes
mainPalette = brewer.pal(8,"Dark2")
```

```{r global_options, include=FALSE}
# make the images flow nicely
knitr::opts_chunk$set(fig.pos = 'H')
```


## Background

In an industry as competitive as Telecom, leading companies know that the key to success is not just about acquiring new customers, but rather, retaining existing ones. However, how do you know which customers tend to churn, and which negative experiences and interactions have the biggest impact on churn.


## Objective

The objective of this research is to find a supervised, binary classification model that would provide accurate forecast of telco customer churn.

# Data Analysis

The data set we are going to use for our research contains  customer’s attributes. There are over 7044 records. It has been sourced from [Kaggle](https://www.kaggle.com/blastchar/telco-customer-churn).


## Data Dictionary



Column Name            | Column Description  
-----------------------| ------------------- 
customerID             | Customer ID 
gender                 | Whether the customer is a male or a female
SeniorCitizen          | Whether the customer is a senior citizen or not (1, 0)
Partner                | Whether the customer has a partner or not (Yes, No)
Dependents             | Whether the customer has dependents or not (Yes, No)
tenure                 | Number of months the customer has stayed with the company
PhoneService           | Whether the customer has a phone service or not (Yes, No)
MultipleLines          | Whether the customer has multiple lines or not (Yes, No, No phone service)
InternetService        | Customer’s internet service provider (DSL, Fiber optic, No)
OnlineSecurity         | Whether the customer has online security or not (Yes, No, No internet service)
OnlineBackup           | Whether the customer has online backup or not (Yes, No, No internet service)
DeviceProtection       | Whether the customer has device protection or not (Yes, No, No internet service)
TechSupport            | Whether the customer has tech support or not (Yes, No, No internet service)
StreamingTV            | Whether the customer has streaming TV or not (Yes, No, No internet service)
StreamingMovies        | Whether the customer has streaming movies or not (Yes, No, No internet service)
Contract               | The contract term of the customer (Month-to-month, One year, Two year)
PaperlessBilling       | Whether the customer has paperless billing or not (Yes, No)
PaymentMethod          | The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))
MonthlyCharges         | The amount charged to the customer monthly
TotalCharges           | The total amount charged to the customer
Churn                  | Whether the customer churned or not (Yes or No)


## Data Exploration

Let's take a close look at the data set.

```{r message=FALSE, warning=FALSE}
customerData = read.csv("../data/WA_Fn-UseC_-Telco-Customer-Churn.csv", header = TRUE, 
               na.strings = c("NA","","#NA"),sep=",")

```
  

To have the full picture of the data let's print the data summary and sample.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
print(xtable(summary(customerData[,1:8])), include.rownames = FALSE, scalebox=.7)
print(xtable(summary(customerData[,9:15])), include.rownames = FALSE, scalebox=.7)
print(xtable(summary(customerData[,16:21]), caption = "\\tt telco customer churn data Summary", 
             label = "data_head"), include.rownames = FALSE, scalebox=.7)

#Display the dataset structure and sumary
str(customerData)

```
\newpage

Display first rows of the dataset

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
print(xtable(customerData[1:10,1:11]), scalebox=.6)
print (xtable(customerData[1:10,12:21],
  caption = "\\tt telco customer churn data", label = "data_head"), include.rownames = F,
  scalebox = .6)
```


Display the continuous variables and summarise it to find NA's

```{r}
library(dplyr)
continues <- select_if(customerData, is.numeric)
summary(continues)
```

There are some NA's so let's find more information about it
```{r  }
is.na(customerData$TotalCharges)
which(is.na(customerData$TotalCharges))
```

NA's are in the 489, 754, 937, 1083, 1341, 3332, 3827, 4381, 5219, 6671 y 6755 rows, let's check each one to analyse how to deal with them.

```{r}
customerData[489,]
customerData[754,]
customerData[937,]
customerData[1083,]
customerData[1341,]
customerData[3332,]
customerData[3827,]
customerData[4381,]
customerData[5219,]
customerData[6671,]
customerData[6755,]

```

All na's in TotalCharges column is because the tenure is 0. so we will change all the na's for 0's

#Factor columns

Display the factor columns and summarise it
```{r}
factorColumns <- select_if(customerData, is.factor)
summary(factorColumns)
```

Now we are going to display factor columns and how the model converts those in dummy variables

```{r}
contrasts(customerData$gender)
contrasts(customerData$Partner)
contrasts(customerData$Dependents)
contrasts(customerData$PhoneService)
contrasts(customerData$MultipleLines)
contrasts(customerData$InternetService)
contrasts(customerData$OnlineSecurity)
contrasts(customerData$OnlineBackup)
contrasts(customerData$DeviceProtection)
contrasts(customerData$TechSupport)
contrasts(customerData$StreamingTV)
contrasts(customerData$StreamingMovies)
contrasts(customerData$Contract)
contrasts(customerData$PaperlessBilling)
contrasts(customerData$PaymentMethod)

```


Make a chart for each factor type column.

```{r test1,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=gender)) +
                        geom_bar()

```

```{r test2,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=Partner)) +
        geom_bar()
```

```{r test3,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=Dependents)) +
        geom_bar()
```

```{r test4,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=PhoneService)) +
        geom_bar()
```

```{r test5,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=MultipleLines)) +
        geom_bar()
```

```{r test6,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=InternetService)) +
        geom_bar()
```

```{r test7,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=OnlineSecurity)) +
        geom_bar()
```


```{r test8,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=OnlineBackup)) +
        geom_bar()
```


```{r test9,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=DeviceProtection)) +
        geom_bar()
```


```{r test10,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=TechSupport)) +
        geom_bar()
```


```{r test11,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=StreamingTV)) +
        geom_bar()
```


```{r test12,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=StreamingMovies)) +
        geom_bar()
```

```{r test13,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=Contract)) +
        geom_bar()
```


```{r test14,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=PaperlessBilling)) +
        geom_bar()
```


```{r test15,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=PaymentMethod)) +
        geom_bar()
```


```{r test16,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(factorColumns, aes(x=Churn)) +
        geom_bar()
```


Now take a look of numeric columns in a graph to better understanding

```{r test17,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(continues, aes(x=tenure)) + 
        geom_density(alpha= .2, fill="#FF6666")

```

```{r test18,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(continues, aes(x=MonthlyCharges)) +
        geom_density(alpha= .2, fill="#FF6666")
```

```{r test19,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" ", out.width="1.1\\linewidth"}
ggplot(continues, aes(x=TotalCharges)) +
        geom_density(alpha= .2, fill="#FF6666")
```


##DATA MANIPULATION

First, create a copy of the dataset to safely manipulate the data.  

```{r}
scaled_data <- customerData

```

Turn NA's values to 0's

```{r}
scaled_data[is.na(scaled_data)] <- 0

```

Scaling the numeric columns

```{r}
scaled_data$tenure=scale(scaled_data$tenure)
scaled_data$MonthlyCharges=scale(scaled_data$MonthlyCharges)
scaled_data$TotalCharges=scale(scaled_data$TotalCharges)

```

Taking out the customerID column, and copy to cleaneddata

```{r}
drop<-c("customerID")
cleaneddata <- scaled_data[,!(names(scaled_data)%in%drop)]

```


Split into train and test data ramdomly before modeling

```{r}
library(caTools)
set.seed(123)
data_sample = sample.split(cleaneddata,SplitRatio=0.80)
train_data = subset(cleaneddata,data_sample==TRUE)
test_data = subset(cleaneddata,data_sample==FALSE)
dim(train_data)
dim(test_data)

```



# Modeling and Evalutation

With the data splitted we can run the first model.

As we need a classification model to predict whether the clients churn, we can run a logistic regression model. first with all variables and then we will run it after feature selection.

## Logistic Regression

```{r}

Logistic_Model=glm(Churn~.,train_data,family=binomial())
summary(Logistic_Model)

```
We have some insights with the summary of the first model: we can see that the variables: gender, seniro citizen, partner,dependents, phone service, Multiplelines, Internetservice, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, streamingMovies, and payment method are not very significant.

The most significant variables are tenure, contract, PaperlessBilling, PaymentMethod y TotalCharges.

AIC for this model is 4619.1


# Feature Selection


Then, try a second model without the variables that according to the first model are less important.

```{r}

Logistic_Model2=glm(Churn~ + tenure + Contract + PaperlessBilling + PaymentMethod 
                    + MonthlyCharges + TotalCharges,train_data,family=binomial())
summary(Logistic_Model2)

```

With this model we obtained an AIC of 4731.8, which is greater than the first one, so it seems not to be better. 


To make a better featue selection we can use some algorithms that help us to choose the correct features to the model.

## Step wise Forward and Backward Selection 

First we create a base model with just the intercept and summarize it.

```{r}
base.mod <- glm(Churn ~ 1, train_data, family = binomial())
summary(base.mod)

```

Now we perform the algorithm with both models with just the intercept and with all of the features.

```{r}
stepMod <- step(base.mod, scope = list(lower = base.mod, upper = Logistic_Model), direction = "both", trace = 0, steps = 1000) 

summary(stepMod)
```


According to the results of this algorithm of feature selection, we can see that the first model with all of the features is a good model and is not a good idea to discard any one.

## Recursive Feature Elimination RFE

We can use another approach to feature selection in order to compare the results and decide.



```{r}
predictors = subset(cleaneddata,select = -Churn)
label = cleaneddata[,20]

# run the RFE algorithm
rfePrediction = rfe(predictors, label, sizes=c(1:19), 
                    rfeControl = rfeControl(functions=rfFuncs, method="cv", number=3))
print(rfePrediction)
```


```{r plot_feature_selection,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Number of Predictors vs Accuracy", out.width="1.1\\linewidth"}
plot(rfePrediction, type=c("g", "o"))
```


Figure \ref{fig:plot_feature_selection}  shows that ...... XXXXXXXXXXXXXXXXxxXXX.

```{r echo=FALSE}
print(predictors(rfePrediction))
```


```{r include=FALSE}
len = length(predictors(rfePrediction))
selectedPredictors =  predictors(rfePrediction)[1:ifelse(len < 8, len, 8)]
print(selectedPredictors)
# remove useless variables
rm(label,predictors,rfePrediction,len)
```




## Decision Tree

Now we are going to fit with a decision tree model. This model is also a good option as can deal better with a unbalanced dataset. 

```{r}
library(rpart)
library(rpart.plot)
decisionTree_model <- rpart(Churn  ~. , train_data, method = 'class')

```

Display the importance of the variables according to this model:

```{r}

decisionTree_model$variable.importance

```


Now lets check the cross validation results of this model:

```{r}

printcp(decisionTree_model)

```

Run the model with the test data

```{r}
predicted_val <- predict(decisionTree_model, test_data, type = 'class')
rpart.plot(decisionTree_model)

```




## Evaluating the performance of each model

# Compare the AUC of models

# Logistic regression model

```{r}

library(pROC)
logr.predict <- predict(Logistic_Model,test_data, probability = TRUE)
auc.lr = roc(test_data$Churn, logr.predict, plot = TRUE, col = "blue")
auc.lr

```

The Area Under Curve is 0.8265

# Decision Tree model

```{r}

auc.DecisionTree = roc(test_data$Churn, factor(predicted_val, ordered = TRUE), plot = TRUE, col = "blue")
auc.DecisionTree

```

The Area Under Curve is 0.6766


# Usig a confusion matrix

# Confusion matrix for logistic regression

```{r}
predictors <- predict(Logistic_Model, test_data, type = 'response')
confMat <- table(test_data$Churn, predictors > 0.5)
confMat


```

Testing the accuracy

```{r}

accuracy_test <- sum(diag(confMat)) / sum(confMat)
accuracy_test

```

The accuracy of the model is 78.9%



# Confusion matrix for logistic regression








## Model Comparison

Choose the better model.





# Conclusion

We need to make conclussions



We consider the project to be a success. 
\newpage





This file was generated using [_The R Journal_ style article template](https://github.com/rstudio/rticles), additional information on how to prepare articles for submission is here - [Instructions for Authors](https://journal.r-project.org/share/author-guide.pdf). The article itself is an executable R Markdown file that could be [downloaded from Github](https://github.com/ivbsoftware/big-data-final-2/blob/master/docs/R_Journal/big-data-final-2/) with all the necessary artifacts.

